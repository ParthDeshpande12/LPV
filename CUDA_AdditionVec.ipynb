{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1712177405238,"user":{"displayName":"Nikita Dhobale","userId":"10972762059524236436"},"user_tz":-330},"id":"e_sLHXVntbEU","outputId":"79097325-a606-4d1e-c6fb-649dc637de59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing Addv2.cu\n"]}],"source":["%%writefile Addv2.cu\n","/*\n"," * A simplified example of vector addition in CUDA to illustrate the\n"," * data decomposition pattern using blocks of threads.\n"," *\n"," * To compile:\n"," *   nvcc -o va-GPU-simple VA-GPU-simple.cu\n"," */\n","\n","#include <stdio.h>\n","\n","// In this example we use a very small number of blocks\n","// and threads in those blocks for illustration\n","// on a very small array\n","#define N 8\n","#define numThread 2 // 2 threads in a block\n","#define numBlock 4  // 4 blocks\n","\n","/*\n"," * 1.\n"," *  The 'kernel' function that will be executed on the GPU device hardware.\n"," */\n","__global__ void add( int *a, int *b, int *c ) {\n","\n","    // the initial index that this thread will work on\n","    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","    // In this above example code, we assume a linear set of blocks of threads in the 'x' dimension,\n","    // which is declared in main below when we run this function.\n","\n","    // The actual computation is being done by individual threads\n","    // in each of the blocks.\n","    // e.g. we use 4 blocks and 2 threads per block (8 threads will run in parallel)\n","    //      and our total array size N is 8\n","    //      the thread whose threadIdx.x is 0 within block 0 will compute c[0],\n","    //          because tid = (2 * 0)  + 0\n","    //      the thread whose threadIdx.x is 0 within block 1 will compute c[2],\n","    //          because tid = (2 * 1) + 0\n","    //      the thread whose threadIdx.x is 1 within block 1 will compute c[3],\n","    //          because tid = (2 * 1) + 1\n","    //\n","    //     The following while loop will execute once for this simple example:\n","    //          c[0] through c[7] will be computed concurrently\n","    //\n","    while (tid < N) {\n","        c[tid] = a[tid] + b[tid];       // The actual computation done by the thread\n","        tid += blockDim.x;       // Increment this thread's index by the number of threads per block:\n","                                 // in this small case, each thread would then have a tid > N\n","    }\n","}\n","\n","\n","/*\n"," * The main program that directs the execution of vector add on the GPU\n"," */\n","int main( void ) {\n","    int *a, *b, *c;               // The arrays on the host CPU machine\n","    int *dev_a, *dev_b, *dev_c;   // The arrays for the GPU device\n","\n","    // 2.a allocate the memory on the CPU\n","    a = (int*)malloc( N * sizeof(int) );\n","    b = (int*)malloc( N * sizeof(int) );\n","    c = (int*)malloc( N * sizeof(int) );\n","\n","    // 2.b. fill the arrays 'a' and 'b' on the CPU with dummy values\n","    for (int i=0; i<N; i++) {\n","        a[i] = 2*i;\n","        b[i] = i*3;\n","    }\n","\n","    // 2.c. allocate the memory on the GPU\n","     cudaMalloc( (void**)&dev_a, N * sizeof(int) );\n","     cudaMalloc( (void**)&dev_b, N * sizeof(int) );\n","     cudaMalloc( (void**)&dev_c, N * sizeof(int) );\n","\n","    // 2.d. copy the arrays 'a' and 'b' to the GPU\n","     cudaMemcpy( dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice );\n","     cudaMemcpy( dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice );\n","\n","    // 3. Execute the vector addition 'kernel function' on th GPU device,\n","    // declaring how many blocks and how many threads per block to use.\n","    add<<<numBlock,numThread>>>( dev_a, dev_b, dev_c );\n","\n","    // 4. copy the array 'c' back from the GPU to the CPU\n","    cudaMemcpy( c, dev_c, N * sizeof(int),cudaMemcpyDeviceToHost );\n","\n","    // verify that the GPU did the work we requested\n","    bool success = true;\n","    int total=0;\n","    printf(\"Checking %d values in the array.\\n\", N);\n","    for (int i=0; i<N; i++) {\n","        if ((a[i] + b[i]) != c[i]) {\n","            printf( \"Error:  %d + %d != %d\\n\", a[i], b[i], c[i] );\n","            success = false;\n","        }\n","        total += 1;\n","    }\n","    if (success)  printf( \"We did it, %d values correct!\\n\", total );\n","\n"," for (int i=0; i<N; i++)\n","       {\n","            printf( \"  %d + %d , =%d\\n\", a[i], b[i], c[i] );\n","\n","    }\n","    // free the memory we allocated on the CPU\n","    free( a );\n","    free( b );\n","    free( c );\n","\n","    // free the memory we allocated on the GPU\n","     cudaFree( dev_a );\n","     cudaFree( dev_b );\n","     cudaFree( dev_c );\n","\n","    return 0;\n","}"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3050,"status":"ok","timestamp":1712177410054,"user":{"displayName":"Nikita Dhobale","userId":"10972762059524236436"},"user_tz":-330},"id":"FwO7dTP7uWTi"},"outputs":[],"source":["!nvcc Addv2.cu -o Add1"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":693,"status":"ok","timestamp":1712177421106,"user":{"displayName":"Nikita Dhobale","userId":"10972762059524236436"},"user_tz":-330},"id":"NCJqaErGud3z","outputId":"9e74898a-7e4f-4fd8-d9f8-37c122d69446"},"outputs":[{"name":"stdout","output_type":"stream","text":["Checking 8 values in the array.\n","We did it, 8 values correct!\n","  0 + 0 , =0\n","  2 + 3 , =5\n","  4 + 6 , =10\n","  6 + 9 , =15\n","  8 + 12 , =20\n","  10 + 15 , =25\n","  12 + 18 , =30\n","  14 + 21 , =35\n"]}],"source":["!./Add1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BYauIYmuhLG"},"outputs":[],"source":["%%writefile Addv2.cu\n","/*\n"," * A simplified example of vector addition in CUDA to illustrate the\n"," * data decomposition pattern using blocks of threads.\n"," *\n"," * To compile:\n"," *   nvcc -o va-GPU-simple VA-GPU-simple.cu\n"," */\n","\n","#include <stdio.h>\n","\n","// In this example we use a very small number of blocks\n","// and threads in those blocks for illustration\n","// on a very small array\n","#define N 8\n","#define numThread 2 // 2 threads in a block\n","#define numBlock 4  // 4 blocks\n","\n","/*\n"," * 1.\n"," *  The 'kernel' function that will be executed on the GPU device hardware.\n"," */\n","__global__ void add( int *a, int *b, int *c ) {\n","\n","    // the initial index that this thread will work on\n","    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","    // In this above example code, we assume a linear set of blocks of threads in the 'x' dimension,\n","    // which is declared in main below when we run this function.\n","\n","    // The actual computation is being done by individual threads\n","    // in each of the blocks.\n","    // e.g. we use 4 blocks and 2 threads per block (8 threads will run in parallel)\n","    //      and our total array size N is 8\n","    //      the thread whose threadIdx.x is 0 within block 0 will compute c[0],\n","    //          because tid = (2 * 0)  + 0\n","    //      the thread whose threadIdx.x is 0 within block 1 will compute c[2],\n","    //          because tid = (2 * 1) + 0\n","    //      the thread whose threadIdx.x is 1 within block 1 will compute c[3],\n","    //          because tid = (2 * 1) + 1\n","    //\n","    //     The following while loop will execute once for this simple example:\n","    //          c[0] through c[7] will be computed concurrently\n","    //\n","    while (tid < N) {\n","        c[tid] = a[tid] + b[tid];       // The actual computation done by the thread\n","        tid += blockDim.x;       // Increment this thread's index by the number of threads per block:\n","                                 // in this small case, each thread would then have a tid > N\n","    }\n","}\n","\n","\n","/*\n"," * The main program that directs the execution of vector add on the GPU\n"," */\n","int main( void ) {\n","    int *a, *b, *c;               // The arrays on the host CPU machine\n","    int *dev_a, *dev_b, *dev_c;   // The arrays for the GPU device\n","\n","    // 2.a allocate the memory on the CPU\n","    a = (int*)malloc( N * sizeof(int) );\n","    b = (int*)malloc( N * sizeof(int) );\n","    c = (int*)malloc( N * sizeof(int) );\n","\n","    // 2.b. fill the arrays 'a' and 'b' on the CPU with dummy values\n","    for (int i=0; i<N; i++) {\n","        a[i] = 2*i;\n","        b[i] = i*3;\n","    }\n","\n","    // 2.c. allocate the memory on the GPU\n","     cudaMalloc( (void**)&dev_a, N * sizeof(int) );\n","     cudaMalloc( (void**)&dev_b, N * sizeof(int) );\n","     cudaMalloc( (void**)&dev_c, N * sizeof(int) );\n","\n","    // 2.d. copy the arrays 'a' and 'b' to the GPU\n","     cudaMemcpy( dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice );\n","     cudaMemcpy( dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice );\n","\n","    // 3. Execute the vector addition 'kernel function' on th GPU device,\n","    // declaring how many blocks and how many threads per block to use.\n","    add<<<numBlock,numThread>>>( dev_a, dev_b, dev_c );\n","\n","    // 4. copy the array 'c' back from the GPU to the CPU\n","    cudaMemcpy( c, dev_c, N * sizeof(int),cudaMemcpyDeviceToHost );\n","\n","    // verify that the GPU did the work we requested\n","    bool success = true;\n","    int total=0;\n","    printf(\"Checking %d values in the array.\\n\", N);\n","    for (int i=0; i<N; i++) {\n","        if ((a[i] + b[i]) != c[i]) {\n","            printf( \"Error:  %d + %d != %d\\n\", a[i], b[i], c[i] );\n","            success = false;\n","        }\n","        total += 1;\n","    }\n","    if (success)  printf( \"We did it, %d values correct!\\n\", total );\n","\n"," for (int i=0; i<N; i++)\n","       {\n","            printf( \"  %d + %d , =%d\\n\", a[i], b[i], c[i] );\n","\n","    }\n","    // free the memory we allocated on the CPU\n","    free( a );\n","    free( b );\n","    free( c );\n","\n","    // free the memory we allocated on the GPU\n","     cudaFree( dev_a );\n","     cudaFree( dev_b );\n","     cudaFree( dev_c );\n","\n","    return 0;\n","}"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOZOuL5tNHC5kQMQKhtjDZk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
